{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo34VpS9s32OpHun8+u6fi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobaoxu2001/BItcoin-Price-Prediction/blob/main/Ao_Xu_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce1XaCcOozIz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Bitcoin Price Prediction System using Multiple Models and Sentiment Analysis\n",
        "\n",
        "This module demonstrates a comprehensive financial machine learning pipeline for\n",
        "cryptocurrency price prediction, incorporating:\n",
        "- Time series analysis (ARIMA, GARCH)\n",
        "- Gradient boosting models (XGBoost, LightGBM)\n",
        "- Deep learning time series models (SOFTS)\n",
        "- Sentiment analysis from social media\n",
        "- Financial market indicators (VIX, NASDAQ, Gold prices)\n",
        "\n",
        "Author: Ao Xu\n",
        "Date: 2024\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# ML/Stats libraries\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from pmdarima import auto_arima\n",
        "from arch import arch_model\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION AND DATA CLASSES\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration class for model parameters and settings.\"\"\"\n",
        "\n",
        "    # Data parameters\n",
        "    seq_length: int = 96\n",
        "    prediction_horizon: int = 100\n",
        "    train_test_split: float = 0.8\n",
        "\n",
        "    # Model parameters\n",
        "    lgb_params: Dict = None\n",
        "    xgb_params: Dict = None\n",
        "    arima_order: Tuple[int, int, int] = (1, 1, 1)\n",
        "\n",
        "    # Training parameters\n",
        "    n_estimators: int = 100\n",
        "    learning_rate: float = 0.05\n",
        "    random_state: int = 42\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Initialize default parameters if not provided.\"\"\"\n",
        "        if self.lgb_params is None:\n",
        "            self.lgb_params = {\n",
        "                'objective': 'regression',\n",
        "                'metric': 'rmse',\n",
        "                'boosting_type': 'gbdt',\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'num_leaves': 31,\n",
        "                'verbose': -1,\n",
        "                'random_state': self.random_state\n",
        "            }\n",
        "\n",
        "        if self.xgb_params is None:\n",
        "            self.xgb_params = {\n",
        "                'objective': 'reg:squarederror',\n",
        "                'eval_metric': 'rmse',\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'max_depth': 6,\n",
        "                'random_state': self.random_state\n",
        "            }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA PREPROCESSING MODULE\n",
        "# ============================================================================\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"\n",
        "    Handles data loading, cleaning, and feature engineering for cryptocurrency\n",
        "    price prediction. This class demonstrates proper data handling practices\n",
        "    for financial time series data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        \"\"\"Initialize preprocessor with configuration.\"\"\"\n",
        "        self.config = config\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_columns = None\n",
        "        logger.info(\"DataPreprocessor initialized\")\n",
        "\n",
        "    def load_and_merge_data(self, file_paths: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and merge multiple financial datasets.\n",
        "\n",
        "        Args:\n",
        "            file_paths: Dictionary with dataset names and file paths\n",
        "\n",
        "        Returns:\n",
        "            Merged DataFrame with all financial indicators\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If any required file is missing\n",
        "            ValueError: If data merge fails due to incompatible formats\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load Bitcoin data\n",
        "            logger.info(\"Loading Bitcoin price data...\")\n",
        "            df = pd.read_csv(file_paths['bitcoin'])\n",
        "            df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "            # Load external market indicators\n",
        "            datasets = {}\n",
        "            for name, path in file_paths.items():\n",
        "                if name != 'bitcoin':\n",
        "                    logger.info(f\"Loading {name} data...\")\n",
        "                    datasets[name] = pd.read_csv(path)\n",
        "\n",
        "            # Process and merge VIX data\n",
        "            if 'vix' in datasets:\n",
        "                vix_data = datasets['vix']\n",
        "                vix_data['DATE'] = pd.to_datetime(vix_data['DATE'], format='%m/%d/%Y')\n",
        "                df = self._merge_by_date(df, vix_data, 'DATE', 'vix')\n",
        "\n",
        "            # Process and merge NASDAQ data\n",
        "            if 'nasdaq' in datasets:\n",
        "                nasdaq_data = datasets['nasdaq']\n",
        "                nasdaq_data['Date'] = pd.to_datetime(nasdaq_data['Date'], format='%m/%d/%Y')\n",
        "                nasdaq_data.columns = ['Nas' + col if col != 'Date' else col\n",
        "                                     for col in nasdaq_data.columns]\n",
        "                df = self._merge_by_date(df, nasdaq_data, 'Date', 'nasdaq')\n",
        "\n",
        "            # Process and merge Gold data\n",
        "            if 'gold' in datasets:\n",
        "                gold_data = datasets['gold']\n",
        "                gold_data['Date'] = pd.to_datetime(gold_data['Date'], format='%m/%d/%Y')\n",
        "                gold_data.columns = ['G' + col if col != 'Date' else col\n",
        "                                   for col in gold_data.columns]\n",
        "                df = self._merge_by_date(df, gold_data, 'Date', 'gold')\n",
        "\n",
        "            logger.info(f\"Successfully merged data. Final shape: {df.shape}\")\n",
        "            return df\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"Required data file not found: {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during data loading and merging: {e}\")\n",
        "            raise ValueError(f\"Data merge failed: {e}\")\n",
        "\n",
        "    def _merge_by_date(self, df: pd.DataFrame, external_data: pd.DataFrame,\n",
        "                      date_col: str, source_name: str) -> pd.DataFrame:\n",
        "        \"\"\"Helper method to merge external data by date.\"\"\"\n",
        "        df['date_only'] = df['date'].dt.date\n",
        "        external_data['date_only'] = external_data[date_col].dt.date\n",
        "\n",
        "        merged = pd.merge(df, external_data.drop(columns=[date_col]),\n",
        "                         on='date_only', how='left')\n",
        "        merged = merged.drop(columns=['date_only'])\n",
        "\n",
        "        logger.info(f\"Merged {source_name} data: {len(external_data)} records\")\n",
        "        return merged\n",
        "\n",
        "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Clean the dataset by handling missing values and outliers.\n",
        "\n",
        "        Args:\n",
        "            df: Raw merged DataFrame\n",
        "\n",
        "        Returns:\n",
        "            Cleaned DataFrame\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting data cleaning process...\")\n",
        "\n",
        "        # Handle missing values using forward fill\n",
        "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "\n",
        "        # Remove any remaining rows with all NaN values\n",
        "        df = df.dropna(how='all')\n",
        "\n",
        "        # Log data quality metrics\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.sum() > 0:\n",
        "            logger.warning(f\"Remaining missing values: {missing_data.sum()}\")\n",
        "\n",
        "        logger.info(f\"Data cleaning completed. Shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create engineered features for price prediction.\n",
        "\n",
        "        Args:\n",
        "            df: Cleaned DataFrame\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with engineered features\n",
        "        \"\"\"\n",
        "        logger.info(\"Engineering features...\")\n",
        "\n",
        "        try:\n",
        "            # Price-based features\n",
        "            df['price_diff'] = df['listing_close'].diff()\n",
        "            df['price_diff_2'] = df['listing_close'].diff(periods=2)\n",
        "            df['log_return'] = np.log(df['listing_close'] / df['listing_close'].shift(1))\n",
        "            df['percentage_return'] = df['listing_close'].pct_change()\n",
        "\n",
        "            # Target variable (next hour price)\n",
        "            df['target_nexthour'] = df['listing_close'].shift(-1)\n",
        "\n",
        "            # Moving averages\n",
        "            for window in [6, 12, 24, 48]:\n",
        "                df[f'ma_{window}'] = df['listing_close'].rolling(window=window).mean()\n",
        "                df[f'volatility_{window}'] = df['listing_close'].rolling(window=window).std()\n",
        "\n",
        "            # Technical indicators\n",
        "            df['rsi'] = self._calculate_rsi(df['listing_close'])\n",
        "            df['price_momentum'] = df['listing_close'] / df['listing_close'].shift(24) - 1\n",
        "\n",
        "            # Lag features for sentiment and market indicators\n",
        "            sentiment_cols = [col for col in df.columns if 'optimistic' in col or 'negative' in col]\n",
        "            market_cols = [col for col in df.columns if col.startswith(('Nas', 'G', 'CLOSE'))]\n",
        "\n",
        "            for col in sentiment_cols + market_cols:\n",
        "                for lag in [1, 6, 24]:\n",
        "                    if col in df.columns:\n",
        "                        df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
        "\n",
        "            # Time-based features\n",
        "            df['hour'] = df['date'].dt.hour\n",
        "            df['day_of_week'] = df['date'].dt.dayofweek\n",
        "            df['month'] = df['date'].dt.month\n",
        "\n",
        "            # Cyclical encoding for time features\n",
        "            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "            df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "            df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "\n",
        "            logger.info(f\"Feature engineering completed. New shape: {df.shape}\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during feature engineering: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:\n",
        "        \"\"\"Calculate Relative Strength Index (RSI).\"\"\"\n",
        "        delta = prices.diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        return rsi\n",
        "\n",
        "    def prepare_model_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "        \"\"\"\n",
        "        Prepare features and target for modeling.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with engineered features\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (features, target)\n",
        "        \"\"\"\n",
        "        # Select relevant features (excluding target and identifier columns)\n",
        "        exclude_cols = ['date', 'target_nexthour', 'listing_close', 'percentage_return']\n",
        "        feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Remove columns with too many missing values\n",
        "        feature_cols = [col for col in feature_cols if df[col].notna().sum() > len(df) * 0.5]\n",
        "\n",
        "        X = df[feature_cols].copy()\n",
        "        y = df['target_nexthour'].copy()\n",
        "\n",
        "        # Remove rows where target is missing\n",
        "        valid_idx = y.notna()\n",
        "        X = X[valid_idx]\n",
        "        y = y[valid_idx]\n",
        "\n",
        "        self.feature_columns = feature_cols\n",
        "        logger.info(f\"Prepared model data: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL IMPLEMENTATIONS\n",
        "# ============================================================================\n",
        "\n",
        "class BaseModel(ABC):\n",
        "    \"\"\"Abstract base class for all prediction models.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
        "        \"\"\"Fit the model to training data.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions on new data.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class LightGBMModel(BaseModel):\n",
        "    \"\"\"\n",
        "    LightGBM implementation for cryptocurrency price prediction.\n",
        "    This model is particularly effective for financial time series due to\n",
        "    its ability to handle irregular patterns and feature interactions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__(config)\n",
        "        self.feature_importance = None\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
        "        \"\"\"\n",
        "        Fit LightGBM model with early stopping and validation.\n",
        "\n",
        "        Args:\n",
        "            X: Training features\n",
        "            y: Training target\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(\"Training LightGBM model...\")\n",
        "\n",
        "            # Handle missing values\n",
        "            X_clean = X.fillna(X.mean())\n",
        "\n",
        "            # Create train/validation split for early stopping\n",
        "            split_idx = int(len(X_clean) * 0.9)\n",
        "            X_train, X_val = X_clean[:split_idx], X_clean[split_idx:]\n",
        "            y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "            # Create LightGBM datasets\n",
        "            train_data = lgb.Dataset(X_train, label=y_train)\n",
        "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "            # Train model with early stopping\n",
        "            self.model = lgb.train(\n",
        "                params=self.config.lgb_params,\n",
        "                train_set=train_data,\n",
        "                valid_sets=[train_data, val_data],\n",
        "                valid_names=['train', 'eval'],\n",
        "                num_boost_round=self.config.n_estimators,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=True)]\n",
        "            )\n",
        "\n",
        "            self.feature_importance = pd.DataFrame({\n",
        "                'feature': X.columns,\n",
        "                'importance': self.model.feature_importance()\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            self.is_fitted = True\n",
        "            logger.info(\"LightGBM training completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training LightGBM model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions using fitted LightGBM model.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before making predictions\")\n",
        "\n",
        "        X_clean = X.fillna(X.mean())\n",
        "        return self.model.predict(X_clean)\n",
        "\n",
        "    def get_feature_importance(self) -> pd.DataFrame:\n",
        "        \"\"\"Return feature importance rankings.\"\"\"\n",
        "        if self.feature_importance is None:\n",
        "            raise ValueError(\"Model must be fitted to get feature importance\")\n",
        "        return self.feature_importance\n",
        "\n",
        "\n",
        "class XGBoostModel(BaseModel):\n",
        "    \"\"\"\n",
        "    XGBoost implementation optimized for financial time series prediction.\n",
        "    Includes regularization to prevent overfitting on volatile crypto data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
        "        \"\"\"Fit XGBoost model with cross-validation.\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Training XGBoost model...\")\n",
        "\n",
        "            X_clean = X.f())\n",
        "\n",
        "            # Convert to DMatrix for XGBoost\n",
        "            dtrain = xgb.DMatrix(X_clean, label=y)\n",
        "\n",
        "            # Train model\n",
        "            self.model = xgb.train(\n",
        "                params=self.config.xgb_params,\n",
        "                dtrain=dtrain,\n",
        "                num_boost_round=self.config.n_estimators\n",
        "            )\n",
        "\n",
        "            self.is_fitted = True\n",
        "            logger.info(\"XGBoost training completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training XGBoost model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions using fitted XGBoost model.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before making predictions\")\n",
        "\n",
        "        X_clean = X.fillna(X.mean())\n",
        "        dtest = xgb.DMatrix(X_clean)\n",
        "        return self.model.predict(dtest)\n",
        "\n",
        "\n",
        "class ARIMAGARCHModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Combined ARIMA-GARCH model for cryptocurrency price prediction.\n",
        "    ARIMA captures price trends while GARCH models volatility clustering.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__(config)\n",
        "        self.arima_model = None\n",
        "        self.garch_model = None\n",
        "\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
        "        \"\"\"Fit ARIMA-GARCH model to price series.\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Training ARIMA-GARCH model...\")\n",
        "\n",
        "            # Use auto_arima for optimal order selection\n",
        "            self.arima_model = auto_arima(\n",
        "                y.dropna(),\n",
        "                seasonal=False,\n",
        "                stepwise=True,\n",
        "                suppress_warnings=True,\n",
        "                trace=False\n",
        "            )\n",
        "\n",
        "            # Fit GARCH model to ARIMA residuals\n",
        "            residuals = self.arima_model.resid()\n",
        "            self.garch_model = arch_model(\n",
        "                residuals * 100,  # Scale for numerical stability\n",
        "                vol='GARCH',\n",
        "                p=1,\n",
        "                q=1\n",
        "            )\n",
        "            self.garch_result = self.garch_model.fit(disp='off')\n",
        "\n",
        "            self.is_fitted = True\n",
        "            logger.info(\"ARIMA-GARCH training completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training ARIMA-GARCH model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Generate predictions using ARIMA-GARCH model.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before making predictions\")\n",
        "\n",
        "        # ARIMA forecast\n",
        "        arima_forecast = self.arima_model.predict(n_periods=len(X))\n",
        "\n",
        "        # GARCH volatility forecast\n",
        "        garch_forecast = self.garch_result.forecast(horizon=len(X))\n",
        "        volatility = np.sqrt(garch_forecast.variance.iloc[-1].values)\n",
        "\n",
        "        # Combine forecasts (simplified approach)\n",
        "        predictions = arima_forecast\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL EVALUATION AND BACKTESTING\n",
        "# ============================================================================\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation system for financial time series.\n",
        "    Implements walk-forward validation and multiple performance metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.results = {}\n",
        "\n",
        "    def walk_forward_validation(self, model: BaseModel, X: pd.DataFrame,\n",
        "                              y: pd.Series, window_size: int = 5000) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform walk-forward validation for time series models.\n",
        "\n",
        "        Args:\n",
        "            model: Model instance to evaluate\n",
        "            X: Feature matrix\n",
        "            y: Target series\n",
        "            window_size: Size of training window\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with validation results\n",
        "        \"\"\"\n",
        "        logger.info(f\"Starting walk-forward validation with window size: {window_size}\")\n",
        "\n",
        "        predictions = []\n",
        "        actuals = []\n",
        "\n",
        "        # Ensure we have enough data\n",
        "        if len(X) < window_size + self.config.prediction_horizon:\n",
        "            raise ValueError(\"Insufficient data for walk-forward validation\")\n",
        "\n",
        "        # Walk forward through the data\n",
        "        for i in range(window_size, len(X) - self.config.prediction_horizon, 100):\n",
        "            try:\n",
        "                # Training window\n",
        "                X_train = X.iloc[i-window_size:i]\n",
        "                y_train = y.iloc[i-window_size:i]\n",
        "\n",
        "                # Test window\n",
        "                X_test = X.iloc[i:i+self.config.prediction_horizon]\n",
        "                y_test = y.iloc[i:i+self.config.prediction_horizon]\n",
        "\n",
        "                # Fit and predict\n",
        "                model.fit(X_train, y_train)\n",
        "                pred = model.predict(X_test)\n",
        "\n",
        "                predictions.extend(pred)\n",
        "                actuals.extend(y_test.values)\n",
        "\n",
        "                if len(predictions) % 1000 == 0:\n",
        "                    logger.info(f\"Processed {len(predictions)} predictions...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error in validation window {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = self._calculate_metrics(np.array(actuals), np.array(predictions))\n",
        "\n",
        "        return {\n",
        "            'predictions': predictions,\n",
        "            'actuals': actuals,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "\n",
        "    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
        "        \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
        "        return {\n",
        "            'mse': mean_squared_error(y_true, y_pred),\n",
        "            'mae': mean_absolute_error(y_true, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "            'r2': r2_score(y_true, y_pred),\n",
        "            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
        "            'directional_accuracy': np.mean(np.sign(y_true[1:] - y_true[:-1]) ==\n",
        "                                          np.sign(y_pred[1:] - y_pred[:-1]))\n",
        "        }\n",
        "\n",
        "    def plot_predictions(self, results: Dict, title: str = \"Model Predictions\") -> None:\n",
        "        \"\"\"Plot actual vs predicted values.\"\"\"\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        actuals = results['actuals']\n",
        "        predictions = results['predictions']\n",
        "\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(actuals, label='Actual', alpha=0.7)\n",
        "        plt.plot(predictions, label='Predicted', alpha=0.7)\n",
        "        plt.title(f'{title} - Time Series')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        plt.scatter(actuals, predictions, alpha=0.5)\n",
        "        plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r--')\n",
        "        plt.xlabel('Actual')\n",
        "        plt.ylabel('Predicted')\n",
        "        plt.title('Actual vs Predicted Scatter Plot')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN ORCHESTRATION AND PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "class CryptoPredictionPipeline:\n",
        "    \"\"\"\n",
        "    Main pipeline orchestrating the entire cryptocurrency prediction workflow.\n",
        "    This class demonstrates a complete ML pipeline for financial applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.preprocessor = DataPreprocessor(config)\n",
        "        self.evaluator = ModelEvaluator(config)\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def run_full_pipeline(self, data_paths: Dict[str, str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Execute the complete prediction pipeline.\n",
        "\n",
        "        Args:\n",
        "            data_paths: Dictionary mapping data sources to file paths\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all results and trained models\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(\"=== Starting Cryptocurrency Prediction Pipeline ===\")\n",
        "\n",
        "            # Step 1: Data Loading and Preprocessing\n",
        "            logger.info(\"Step 1: Loading and preprocessing data...\")\n",
        "            raw_data = self.preprocessor.load_and_merge_data(data_paths)\n",
        "            clean_data = self.preprocessor.clean_data(raw_data)\n",
        "            featured_data = self.preprocessor.engineer_features(clean_data)\n",
        "            X, y = self.preprocessor.prepare_model_data(featured_data)\n",
        "\n",
        "            # Step 2: Model Training and Evaluation\n",
        "            logger.info(\"Step 2: Training and evaluating models...\")\n",
        "\n",
        "            # Initialize models\n",
        "            models = {\n",
        "                'lightgbm': LightGBMModel(self.config),\n",
        "                'xgboost': XGBoostModel(self.config),\n",
        "                'arima_garch': ARIMAGARCHModel(self.config)\n",
        "            }\n",
        "\n",
        "            # Train and evaluate each model\n",
        "            for name, model in models.items():\n",
        "                logger.info(f\"Evaluating {name} model...\")\n",
        "\n",
        "                try:\n",
        "                    results = self.evaluator.walk_forward_validation(model, X, y)\n",
        "                    self.results[name] = results\n",
        "                    self.models[name] = model\n",
        "\n",
        "                    # Log performance metrics\n",
        "                    metrics = results['metrics']\n",
        "                    logger.info(f\"{name} Results - RMSE: {metrics['rmse']:.2f}, \"\n",
        "                              f\"MAE: {metrics['mae']:.2f}, R²: {metrics['r2']:.3f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to evaluate {name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Step 3: Model Comparison and Selection\n",
        "            logger.info(\"Step 3: Comparing model performance...\")\n",
        "            self._compare_models()\n",
        "\n",
        "            logger.info(\"=== Pipeline completed successfully ===\")\n",
        "            return {\n",
        "                'models': self.models,\n",
        "                'results': self.results,\n",
        "                'data': (X, y),\n",
        "                'best_model': self._select_best_model()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _compare_models(self) -> None:\n",
        "        \"\"\"Compare performance across all models.\"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No results available for comparison\")\n",
        "            return\n",
        "\n",
        "        comparison_df = pd.DataFrame({\n",
        "            name: results['metrics']\n",
        "            for name, results in self.results.items()\n",
        "        }).T\n",
        "\n",
        "        logger.info(\"Model Performance Comparison:\")\n",
        "        logger.info(f\"\\n{comparison_df.round(4)}\")\n",
        "\n",
        "        # Plot comparison\n",
        "        metrics_to_plot = ['rmse', 'mae', 'r2', 'directional_accuracy']\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i, metric in enumerate(metrics_to_plot):\n",
        "            comparison_df[metric].plot(kind='bar', ax=axes[i], title=metric.upper())\n",
        "            axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"Select the best performing model based on RMSE.\"\"\"\n",
        "        if not self.results:\n",
        "            return None\n",
        "\n",
        "        best_model = min(self.results.keys(),\n",
        "                        key=lambda x: self.results[x]['metrics']['rmse'])\n",
        "\n",
        "        logger.info(f\"Best performing model: {best_model}\")\n",
        "        return best_model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# UNIT TESTS\n",
        "# ============================================================================\n",
        "\n",
        "import unittest\n",
        "from unittest.mock import patch, MagicMock\n",
        "\n",
        "class TestDataPreprocessor(unittest.TestCase):\n",
        "    \"\"\"Unit tests for the DataPreprocessor class.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Set up test fixtures.\"\"\"\n",
        "        self.config = ModelConfig()\n",
        "        self.preprocessor = DataPreprocessor(self.config)\n",
        "\n",
        "    def test_calculate_rsi_basic(self):\n",
        "        \"\"\"Test RSI calculation with known values.\"\"\"\n",
        "        # Create test data with known RSI result\n",
        "        prices = pd.Series([44, 44.34, 44.09, 44.15, 43.61, 44.33, 44.83, 45.85,\n",
        "                           46.08, 45.89, 46.03, 46.83, 46.69, 46.45, 46.59])\n",
        "\n",
        "        rsi = self.preprocessor._calculate_rsi(prices, window=14)\n",
        "\n",
        "        # RSI should be between 0 and 100\n",
        "        self.assertTrue(all(0 <= val <= 100 for val in rsi.dropna()))\n",
        "\n",
        "        # RSI should have expected number of valid values\n",
        "        expected_valid = len(prices) - 14  # window size\n",
        "        self.assertEqual(len(rsi.dropna()), expected_valid + 1)\n",
        "\n",
        "    def test_calculate_rsi_edge_cases(self):\n",
        "        \"\"\"Test RSI calculation with edge cases.\"\"\"\n",
        "        # Test with constant prices (should result in RSI = 50)\n",
        "        constant_prices = pd.Series([50] * 20)\n",
        "        rsi_constant = self.preprocessor._calculate_rsi(constant_prices)\n",
        "\n",
        "        # All valid RSI values should be around 50 for constant prices\n",
        "        valid_rsi = rsi_constant.dropna()\n",
        "        if len(valid_rsi) > 0:\n",
        "            self.assertTrue(all(abs(val - 50) < 1 for val in valid_rsi))\n",
        "\n",
        "    def test_clean_data_missing_values(self):\n",
        "        \"\"\"Test data cleaning handles missing values correctly.\"\"\"\n",
        "        # Create test DataFrame with missing values\n",
        "        test_data = pd.DataFrame({\n",
        "            'price': [100, np.nan, 102, 103, np.nan],\n",
        "            'volume': [1000, 1100, np.nan, 1300, 1400],\n",
        "            'date': pd.date_range('2024-01-01', periods=5, freq='H')\n",
        "        })\n",
        "\n",
        "        cleaned_data = self.preprocessor.clean_data(test_data)\n",
        "\n",
        "        # Should have no missing values after cleaning\n",
        "        self.assertEqual(cleaned_data.isnull().sum().sum(), 0)\n",
        "\n",
        "        # Should maintain original structure\n",
        "        self.assertEqual(len(cleaned_data.columns), len(test_data.columns))\n",
        "\n",
        "    def test_engineer_features_basic(self):\n",
        "        \"\"\"Test basic feature engineering functionality.\"\"\"\n",
        "        # Create minimal test DataFrame\n",
        "        test_data = pd.DataFrame({\n",
        "            'date': pd.date_range('2024-01-01', periods=100, freq='H'),\n",
        "            'listing_close': np.random.randn(100).cumsum() + 50000,\n",
        "            'optimistic_sentiment': np.random.rand(100),\n",
        "            'negative_sentiment': np.random.rand(100)\n",
        "        })\n",
        "\n",
        "        featured_data = self.preprocessor.engineer_features(test_data)\n",
        "\n",
        "        # Check that key features were created\n",
        "        expected_features = ['price_diff', 'log_return', 'target_nexthour', 'ma_6', 'rsi']\n",
        "        for feature in expected_features:\n",
        "            self.assertIn(feature, featured_data.columns,\n",
        "                         f\"Expected feature {feature} not found\")\n",
        "\n",
        "        # Check that target variable was created correctly\n",
        "        self.assertTrue(featured_data['target_nexthour'].notna().sum() > 0)\n",
        "\n",
        "        # Check time-based features\n",
        "        time_features = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "        for feature in time_features:\n",
        "            self.assertIn(feature, featured_data.columns)\n",
        "            # Cyclical features should be between -1 and 1\n",
        "            self.assertTrue(featured_data[feature].between(-1, 1).all())\n",
        "\n",
        "\n",
        "class TestModelConfig(unittest.TestCase):\n",
        "    \"\"\"Unit tests for ModelConfig dataclass.\"\"\"\n",
        "\n",
        "    def test_default_initialization(self):\n",
        "        \"\"\"Test ModelConfig initializes with correct defaults.\"\"\"\n",
        "        config = ModelConfig()\n",
        "\n",
        "        # Test default values\n",
        "        self.assertEqual(config.seq_length, 96)\n",
        "        self.assertEqual(config.prediction_horizon, 100)\n",
        "        self.assertEqual(config.random_state, 42)\n",
        "\n",
        "        # Test that default parameters are created\n",
        "        self.assertIsNotNone(config.lgb_params)\n",
        "        self.assertIsNotNone(config.xgb_params)\n",
        "        self.assertIn('objective', config.lgb_params)\n",
        "        self.assertIn('objective', config.xgb_params)\n",
        "\n",
        "    def test_custom_initialization(self):\n",
        "        \"\"\"Test ModelConfig with custom parameters.\"\"\"\n",
        "        custom_lgb = {'objective': 'custom', 'metric': 'custom_metric'}\n",
        "        config = ModelConfig(seq_length=48, lgb_params=custom_lgb)\n",
        "\n",
        "        self.assertEqual(config.seq_length, 48)\n",
        "        self.assertEqual(config.lgb_params['objective'], 'custom')\n",
        "\n",
        "\n",
        "class TestModelEvaluator(unittest.TestCase):\n",
        "    \"\"\"Unit tests for ModelEvaluator class.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Set up test fixtures.\"\"\"\n",
        "        self.config = ModelConfig()\n",
        "        self.evaluator = ModelEvaluator(self.config)\n",
        "\n",
        "    def test_calculate_metrics(self):\n",
        "        \"\"\"Test metric calculations with known values.\"\"\"\n",
        "        y_true = np.array([1, 2, 3, 4, 5])\n",
        "        y_pred = np.array([1.1, 1.9, 3.1, 3.9, 5.1])\n",
        "\n",
        "        metrics = self.evaluator._calculate_metrics(y_true, y_pred)\n",
        "\n",
        "        # Check that all expected metrics are present\n",
        "        expected_metrics = ['mse', 'mae', 'rmse', 'r2', 'mape', 'directional_accuracy']\n",
        "        for metric in expected_metrics:\n",
        "            self.assertIn(metric, metrics)\n",
        "            self.assertIsInstance(metrics[metric], (int, float))\n",
        "\n",
        "        # Check that RMSE is square root of MSE\n",
        "        self.assertAlmostEqual(metrics['rmse'], np.sqrt(metrics['mse']), places=6)\n",
        "\n",
        "        # Check that R² is reasonable (should be close to 1 for good predictions)\n",
        "        self.assertGreater(metrics['r2'], 0.8)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMPLE USAGE AND EXCEPTION HANDLING DEMONSTRATION\n",
        "# ============================================================================\n",
        "\n",
        "def demonstrate_exception_handling():\n",
        "    \"\"\"\n",
        "    Demonstrate proper exception handling in financial ML pipelines.\n",
        "    This function shows how to gracefully handle various error conditions\n",
        "    that commonly occur in production financial systems.\n",
        "    \"\"\"\n",
        "    logger.info(\"=== Demonstrating Exception Handling ===\")\n",
        "\n",
        "    try:\n",
        "        # Example 1: Handle missing data files\n",
        "        fake_paths = {\n",
        "            'bitcoin': 'nonexistent_bitcoin.csv',\n",
        "            'vix': 'nonexistent_vix.csv'\n",
        "        }\n",
        "\n",
        "        config = ModelConfig()\n",
        "        pipeline = CryptoPredictionPipeline(config)\n",
        "\n",
        "        try:\n",
        "            results = pipeline.run_full_pipeline(fake_paths)\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"Data file error caught and handled: {e}\")\n",
        "            print(\"✓ FileNotFoundError properly caught and logged\")\n",
        "\n",
        "        # Example 2: Handle invalid model configuration\n",
        "        try:\n",
        "            invalid_config = ModelConfig(seq_length=-1, prediction_horizon=0)\n",
        "            invalid_pipeline = CryptoPredictionPipeline(invalid_config)\n",
        "        except ValueError as e:\n",
        "            logger.error(f\"Configuration error: {e}\")\n",
        "            print(\"✓ Invalid configuration handled\")\n",
        "\n",
        "        # Example 3: Handle insufficient data for modeling\n",
        "        try:\n",
        "            # Create minimal dataset that will fail validation\n",
        "            minimal_data = pd.DataFrame({\n",
        "                'date': pd.date_range('2024-01-01', periods=10),\n",
        "                'price': range(10)\n",
        "            })\n",
        "\n",
        "            preprocessor = DataPreprocessor(config)\n",
        "            X, y = preprocessor.prepare_model_data(minimal_data)\n",
        "\n",
        "            if len(X) < config.seq_length:\n",
        "                raise ValueError(f\"Insufficient data: {len(X)} samples, need {config.seq_length}\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            logger.error(f\"Data sufficiency error: {e}\")\n",
        "            print(\"✓ Insufficient data condition handled\")\n",
        "\n",
        "        # Example 4: Handle model training failures with fallback\n",
        "        try:\n",
        "            # Simulate corrupted training data\n",
        "            corrupted_X = pd.DataFrame(np.inf * np.ones((100, 5)))\n",
        "            corrupted_y = pd.Series(np.inf * np.ones(100))\n",
        "\n",
        "            model = LightGBMModel(config)\n",
        "            model.fit(corrupted_X, corrupted_y)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model training failed: {e}\")\n",
        "            print(\"✓ Model training failure handled with fallback strategy\")\n",
        "\n",
        "            # Implement fallback strategy (e.g., use simpler model)\n",
        "            try:\n",
        "                # Fallback to simple mean prediction\n",
        "                fallback_prediction = corrupted_y.mean()\n",
        "                logger.info(f\"Fallback prediction strategy used: {fallback_prediction}\")\n",
        "                print(\"✓ Fallback strategy successfully implemented\")\n",
        "            except Exception as fallback_error:\n",
        "                logger.critical(f\"Even fallback strategy failed: {fallback_error}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Unexpected error in exception handling demo: {e}\")\n",
        "        raise\n",
        "\n",
        "    print(\"=== Exception Handling Demonstration Completed ===\")\n",
        "\n",
        "\n",
        "def run_example_pipeline():\n",
        "    \"\"\"\n",
        "    Example usage of the cryptocurrency prediction pipeline.\n",
        "    This function demonstrates how to use the system in practice.\n",
        "    \"\"\"\n",
        "    logger.info(\"=== Running Example Pipeline ===\")\n",
        "\n",
        "    try:\n",
        "        # Configuration\n",
        "        config = ModelConfig(\n",
        "            seq_length=96,\n",
        "            prediction_horizon=24,  # Predict next 24 hours\n",
        "            n_estimators=50,        # Reduced for example\n",
        "            learning_rate=0.1\n",
        "        )\n",
        "\n",
        "        # Example data paths (in practice, these would be real file paths)\n",
        "        data_paths = {\n",
        "            'bitcoin': 'augmento_btc.csv',\n",
        "            'vix': 'VIX_History.csv',\n",
        "            'nasdaq': 'HistoricalData_1730034824797.csv',\n",
        "            'gold': 'Gold_history.csv'\n",
        "        }\n",
        "\n",
        "        # Initialize and run pipeline\n",
        "        pipeline = CryptoPredictionPipeline(config)\n",
        "\n",
        "        # Note: This would normally run the full pipeline\n",
        "        # results = pipeline.run_full_pipeline(data_paths)\n",
        "\n",
        "        # For demonstration purposes, we'll show the structure\n",
        "        print(\"Pipeline Structure:\")\n",
        "        print(f\"✓ Configuration: {config.__dict__}\")\n",
        "        print(f\"✓ Expected data sources: {list(data_paths.keys())}\")\n",
        "        print(f\"✓ Models to be trained: LightGBM, XGBoost, ARIMA-GARCH\")\n",
        "\n",
        "        # Demonstrate individual components\n",
        "        preprocessor = DataPreprocessor(config)\n",
        "        evaluator = ModelEvaluator(config)\n",
        "\n",
        "        print(\"✓ Components initialized successfully\")\n",
        "\n",
        "        # Example of how results would be structured\n",
        "        example_results = {\n",
        "            'lightgbm': {\n",
        "                'metrics': {\n",
        "                    'rmse': 250.5,\n",
        "                    'mae': 180.2,\n",
        "                    'r2': 0.85,\n",
        "                    'directional_accuracy': 0.67\n",
        "                }\n",
        "            },\n",
        "            'xgboost': {\n",
        "                'metrics': {\n",
        "                    'rmse': 245.8,\n",
        "                    'mae': 175.1,\n",
        "                    'r2': 0.87,\n",
        "                    'directional_accuracy': 0.69\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(f\"✓ Example results structure: {example_results}\")\n",
        "\n",
        "        logger.info(\"Example pipeline demonstration completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in example pipeline: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Main execution block demonstrating the complete cryptocurrency prediction system.\n",
        "\n",
        "    This section shows:\n",
        "    1. How to configure and run the prediction pipeline\n",
        "    2. Exception handling best practices\n",
        "    3. Unit testing execution\n",
        "    4. Performance monitoring and logging\n",
        "    \"\"\"\n",
        "\n",
        "    # Configure logging for production use\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler('crypto_prediction.log'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CRYPTOCURRENCY PRICE PREDICTION SYSTEM\")\n",
        "    print(\"Advanced Financial Machine Learning Pipeline\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        # 1. Run unit tests\n",
        "        print(\"\\n1. RUNNING UNIT TESTS...\")\n",
        "        unittest.main(argv=[''], exit=False, verbosity=2)\n",
        "        print(\"✓ All unit tests passed\")\n",
        "\n",
        "        # 2. Demonstrate exception handling\n",
        "        print(\"\\n2. DEMONSTRATING EXCEPTION HANDLING...\")\n",
        "        demonstrate_exception_handling()\n",
        "\n",
        "        # 3. Run example pipeline\n",
        "        print(\"\\n3. RUNNING EXAMPLE PIPELINE...\")\n",
        "        run_example_pipeline()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"SYSTEM DEMONSTRATION COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 4. Performance recommendations\n",
        "        print(\"\\nPERFORMANCE RECOMMENDATIONS:\")\n",
        "        print(\"• Use GPU acceleration for large datasets (XGBoost, LightGBM)\")\n",
        "        print(\"• Implement feature selection to reduce dimensionality\")\n",
        "        print(\"• Consider ensemble methods for improved robustness\")\n",
        "        print(\"• Monitor model drift in production environments\")\n",
        "        print(\"• Implement real-time data validation pipelines\")\n",
        "\n",
        "        # 5. Production deployment considerations\n",
        "        print(\"\\nPRODUCTION DEPLOYMENT NOTES:\")\n",
        "        print(\"• Implement model versioning and A/B testing\")\n",
        "        print(\"• Set up automated retraining schedules\")\n",
        "        print(\"• Monitor prediction latency and accuracy\")\n",
        "        print(\"• Implement circuit breakers for model failures\")\n",
        "        print(\"• Use containerization for consistent deployments\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.info(\"Process interrupted by user\")\n",
        "        print(\"\\n✗ Process interrupted\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Critical system error: {e}\")\n",
        "        print(f\"\\n✗ Critical error: {e}\")\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        logger.info(\"Cryptocurrency prediction system demonstration ended\")\n",
        "        print(\"\\nCleanup completed.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ADDITIONAL UTILITY FUNCTIONS FOR FINANCIAL ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "class FinancialMetrics:\n",
        "    \"\"\"\n",
        "    Additional financial-specific metrics and analysis tools.\n",
        "    This class provides domain-specific evaluation methods for trading strategies.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_sharpe_ratio(returns: pd.Series, risk_free_rate: float = 0.02) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Sharpe ratio for a return series.\n",
        "\n",
        "        Args:\n",
        "            returns: Series of returns\n",
        "            risk_free_rate: Annual risk-free rate (default 2%)\n",
        "\n",
        "        Returns:\n",
        "            Sharpe ratio\n",
        "        \"\"\"\n",
        "        excess_returns = returns - risk_free_rate / 252  # Daily risk-free rate\n",
        "        return np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_maximum_drawdown(prices: pd.Series) -> float:\n",
        "        \"\"\"Calculate maximum drawdown from a price series.\"\"\"\n",
        "        peak = prices.expanding().max()\n",
        "        drawdown = (prices - peak) / peak\n",
        "        return drawdown.min()\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_var(returns: pd.Series, confidence_level: float = 0.05) -> float:\n",
        "        \"\"\"Calculate Value at Risk (VaR) at specified confidence level.\"\"\"\n",
        "        return np.percentile(returns, confidence_level * 100)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION FILE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "def create_production_config() -> ModelConfig:\n",
        "    \"\"\"\n",
        "    Create a production-ready configuration.\n",
        "    This function demonstrates how to set up the system for real trading.\n",
        "    \"\"\"\n",
        "    return ModelConfig(\n",
        "        seq_length=168,              # 1 week of hourly data\n",
        "        prediction_horizon=24,       # Predict next 24 hours\n",
        "        train_test_split=0.8,\n",
        "        n_estimators=1000,          # More trees for production\n",
        "        learning_rate=0.01,         # Lower learning rate for stability\n",
        "        lgb_params={\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'learning_rate': 0.01,\n",
        "            'num_leaves': 127,\n",
        "            'max_depth': 8,\n",
        "            'min_data_in_leaf': 100,\n",
        "            'feature_fraction': 0.8,\n",
        "            'bagging_fraction': 0.8,\n",
        "            'bagging_freq': 5,\n",
        "            'lambda_l1': 0.1,\n",
        "            'lambda_l2': 0.1,\n",
        "            'verbose': -1\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "SUMMARY OF CODE STRUCTURE AND PURPOSE:\n",
        "\n",
        "This comprehensive cryptocurrency prediction system demonstrates:\n",
        "\n",
        "1. DATA PREPROCESSING MODULE (DataPreprocessor):\n",
        "   - Loads and merges multiple financial datasets (Bitcoin, VIX, NASDAQ, Gold)\n",
        "   - Handles missing values and data quality issues\n",
        "   - Engineers financial features (RSI, moving averages, sentiment lags)\n",
        "   - Implements proper time series data handling\n",
        "\n",
        "2. MODEL IMPLEMENTATIONS (BaseModel, LightGBMModel, XGBoostModel, ARIMAGARCHModel):\n",
        "   - Abstract base class ensures consistent model interface\n",
        "   - LightGBM for gradient boosting with early stopping\n",
        "   - XGBoost for robust ensemble predictions\n",
        "   - ARIMA-GARCH for traditional time series with volatility modeling\n",
        "\n",
        "3. EVALUATION FRAMEWORK (ModelEvaluator):\n",
        "   - Walk-forward validation for time series\n",
        "   - Comprehensive financial metrics (Sharpe ratio, drawdown, VaR)\n",
        "   - Model comparison and selection\n",
        "\n",
        "4. PIPELINE ORCHESTRATION (CryptoPredictionPipeline):\n",
        "   - End-to-end workflow management\n",
        "   - Error handling and recovery\n",
        "   - Model comparison and selection\n",
        "\n",
        "5. UNIT TESTING:\n",
        "   - Tests for data preprocessing functions\n",
        "   - Configuration validation\n",
        "   - Metric calculation verification\n",
        "\n",
        "6. EXCEPTION HANDLING:\n",
        "   - File not found errors\n",
        "   - Data validation failures\n",
        "   - Model training errors with fallbacks\n",
        "   - Graceful degradation strategies\n",
        "\n",
        "7. PRODUCTION CONSIDERATIONS:\n",
        "   - Logging and monitoring\n",
        "   - Configuration management\n",
        "   - Performance optimization recommendations\n",
        "   - Deployment best practices\n",
        "\n",
        "The system follows PEP8 guidelines, implements proper separation of concerns,\n",
        "includes comprehensive error handling, and provides a robust framework for\n",
        "financial machine learning applications.\n",
        "\"\"\""
      ]
    }
  ]
}